{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.util.platform import start_spark, start_spark_local\n",
    "from pipeline.util.storage import read_parquet, write_parquet, write_repart_parquet\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, HashingTF, MinHashLSH, RegexTokenizer, SQLTransformer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "spark, loggrer, conf = start_spark(app_name='addessmatch_etl_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abmatch = read_parquet(spark, \"{storage_curated}/data/in/addressbase\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformation (51813 addresses left):\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(ADDRESS) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(df_abmatch)\n",
    "\n",
    "df_abmatchtrans = model.transform(df_abmatch)\n",
    "df_abmatchtrans = df_abmatchtrans.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({df_abmatchtrans.count()} addresses left):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bldadr = read_parquet(spark, \"{storage_curated}/data/in/buildingaddress\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformation (331 buildings left):\n"
     ]
    }
   ],
   "source": [
    "# Use pipeline previous defined\n",
    "df_bldadrtrans = model.transform(df_bldadr)\n",
    "df_bldadrtrans = df_bldadrtrans.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({df_bldadrtrans.count()} buildings left):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4810 matches\n"
     ]
    }
   ],
   "source": [
    "df_match = model.stages[-1].approxSimilarityJoin(df_bldadrtrans, df_abmatchtrans, 0.5, \"jaccardDist\")\n",
    "print(f\"{df_match.count()} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "owin = W.Window.partitionBy(df_match.datasetA.id).orderBy(df_match.jaccardDist)\n",
    "df_match = df_match.withColumn(\"oid\",F.row_number().over(owin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matchres = (df_match\n",
    "    .select('datasetA.id',F.col('datasetA.address').alias(\"buildingaddress\"),'datasetB.UPRN','jaccardDist','oid')\n",
    "    .filter(F.col(\"oid\")==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet(spark, df_matchres, \"{storage_curated}/data/in/addressmatchres\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+------------+-------------------+---+\n",
      "|id |buildingaddress                                                           |UPRN        |jaccardDist        |oid|\n",
      "+---+--------------------------------------------------------------------------+------------+-------------------+---+\n",
      "|1  |The South Lawn Medical Practice, S Lawn Terrace, Heavitree, Exeter EX1 2RX|100041046145|0.39344262295081966|1  |\n",
      "|4  |FLAT 52D MORTIMER HOUSE GRENDON ROAD EX1 2NL                              |100040215247|0.1428571428571429 |1  |\n",
      "|5  |Unit 3 The Exebridge Centre Exeter EX4 1AH                                |10013043126 |0.40476190476190477|1  |\n",
      "|6  |30, Guildhall Shopping Centre, Exeter EX4 3HJ                             |10013043352 |0.375              |1  |\n",
      "|7  |Market St, Exeter EX1 1BW                                                 |100041123913|0.3666666666666667 |1  |\n",
      "|9  |36 Southernhay E, Exeter EX1 1NX                                          |100041046158|0.32432432432432434|1  |\n",
      "|10 |6 Isleworth Rd, Exeter EX4 1QU                                            |100040218983|0.368421052631579  |1  |\n",
      "|11 |ABBEVILLE CLOSE,EX2 4SJ                                                   |10024662321 |0.3939393939393939 |1  |\n",
      "|13 |ALBERT STREET,EX1 2BH                                                     |100040200120|0.4193548387096774 |1  |\n",
      "|16 |ALFORD CRESCENT,EX1 3LW                                                   |10023116527 |0.4285714285714286 |1  |\n",
      "|20 |ANNE CLOSE,EX4 7DL                                                        |10013043057 |0.48275862068965514|1  |\n",
      "|21 |ASHLEIGH CLOSE,EX4 2BU                                                    |100040201286|0.4571428571428572 |1  |\n",
      "|23 |AVALON CLOSE,EX4 9EF                                                      |100040201547|0.4666666666666667 |1  |\n",
      "|24 |BADON CLOSE,EX4 9EG                                                       |100040201602|0.4666666666666667 |1  |\n",
      "|25 |BARTHOLOMEW STREET EAST,EX4 3BH                                           |10013040306 |0.3076923076923077 |1  |\n",
      "|26 |BARTHOLOMEW ST WEST,EX4 3AL                                               |100041225086|0.42500000000000004|1  |\n",
      "|27 |BEACON LANE,EX4 8BD                                                       |100040202604|0.4838709677419355 |1  |\n",
      "|28 |BEAUFORT ROAD,EX2 9AB                                                     |100041133472|0.4545454545454546 |1  |\n",
      "|30 |BENNETT SQUARE,EX4 8AZ                                                    |100040202984|0.4411764705882353 |1  |\n",
      "|31 |BINFORD CLOSE,EX1 3JZ                                                     |10023116114 |0.4545454545454546 |1  |\n",
      "+---+--------------------------------------------------------------------------+------------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_abmatch.printSchema()\n",
    "#df_matchres.count()\n",
    "#dft = spark.table(\"mytable\")\n",
    "#spark.sql(\"show tables\").show()\n",
    "#df_matchres.show(truncate=False)\n",
    "#df_match.filter(F.col(\"datasetA.id\")==4).sort(F.col(\"jaccardDist\").desc()).show(n=200,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlTrans = SQLTransformer(statement=\"SELECT *, lower(ADDRESS) lower FROM __THIS__\")\n",
    "# df1 = sqlTrans.transform(df_abmatch)\n",
    "# tokenizer = Tokenizer(inputCol=\"lower\", outputCol=\"token\")\n",
    "# df2 = tokenizer.transform(df1)\n",
    "# remover = StopWordsRemover(inputCol=\"token\", outputCol=\"stop\")\n",
    "# df3 = remover.transform(df2)\n",
    "# sqlTrans = SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\")\n",
    "# df4 = sqlTrans.transform(df3)\n",
    "# rtokenizer = RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1)\n",
    "# df5 = rtokenizer.transform(df4)\n",
    "# ngram = NGram(n=2, inputCol=\"char\", outputCol=\"ngram\")\n",
    "# df6 = ngram.transform(df5)\n",
    "# hashtf = HashingTF(inputCol=\"ngram\", outputCol=\"vector\")\n",
    "# df7 = hashtf.transform(df6)\n",
    "# minhash = MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "# model = minhash.fit(df7)\n",
    "# model.setInputCol(\"vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
