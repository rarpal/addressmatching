{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.util.platform import start_spark, start_spark_local\n",
    "from pipeline.util.storage import read_parquet, write_parquet, write_repart_parquet\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, HashingTF, MinHashLSH, RegexTokenizer, SQLTransformer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "spark, loggrer, conf = start_spark(app_name='addessmatch_etl_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abmatch = read_parquet(spark, \"{storage_curated}/data/in/addressbase\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformation (51813 addresses left):\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(ADDRESS) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(df_abmatch)\n",
    "\n",
    "df_abmatchtrans = model.transform(df_abmatch)\n",
    "df_abmatchtrans = df_abmatchtrans.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({df_abmatchtrans.count()} addresses left):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bldadr = read_parquet(spark, \"{storage_curated}/data/in/buildingaddress\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformation (10 buildings left):\n"
     ]
    }
   ],
   "source": [
    "# Use pipeline previous defined\n",
    "df_bldadrtrans = model.transform(df_bldadr)\n",
    "df_bldadrtrans = df_bldadrtrans.filter(F.size(F.col(\"ngram\")) > 0)\n",
    "print(f\"Example transformation ({df_bldadrtrans.count()} buildings left):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 matches\n"
     ]
    }
   ],
   "source": [
    "df_match = model.stages[-1].approxSimilarityJoin(df_bldadrtrans, df_abmatchtrans, 0.5, \"jaccardDist\")\n",
    "print(f\"{df_match.count()} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "owin = W.Window.partitionBy(df_match.datasetA.id).orderBy(df_match.jaccardDist)\n",
    "df_match = df_match.withColumn(\"oid\",F.row_number().over(owin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matchres = (df_match\n",
    "    .select('datasetA.id','datasetA.address','datasetB.UPRN','jaccardDist','oid')\n",
    "    .filter(F.col(\"oid\")==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet(spark, df_matchres, \"{storage_curated}/data/in/addressmatchres\".format(storage_curated = spark.conf.get('storage.curated')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_abmatch.printSchema()\n",
    "#df_abmatch.count()\n",
    "#dft = spark.table(\"mytable\")\n",
    "#spark.sql(\"show tables\").show()\n",
    "#df_match.printSchema()\n",
    "#df_match.filter(F.col(\"datasetA.id\")==4).sort(F.col(\"jaccardDist\").desc()).show(n=200,truncate=False)\n",
    "#df_abmatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlTrans = SQLTransformer(statement=\"SELECT *, lower(ADDRESS) lower FROM __THIS__\")\n",
    "# df1 = sqlTrans.transform(df_abmatch)\n",
    "# tokenizer = Tokenizer(inputCol=\"lower\", outputCol=\"token\")\n",
    "# df2 = tokenizer.transform(df1)\n",
    "# remover = StopWordsRemover(inputCol=\"token\", outputCol=\"stop\")\n",
    "# df3 = remover.transform(df2)\n",
    "# sqlTrans = SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\")\n",
    "# df4 = sqlTrans.transform(df3)\n",
    "# rtokenizer = RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1)\n",
    "# df5 = rtokenizer.transform(df4)\n",
    "# ngram = NGram(n=2, inputCol=\"char\", outputCol=\"ngram\")\n",
    "# df6 = ngram.transform(df5)\n",
    "# hashtf = HashingTF(inputCol=\"ngram\", outputCol=\"vector\")\n",
    "# df7 = hashtf.transform(df6)\n",
    "# minhash = MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "# model = minhash.fit(df7)\n",
    "# model.setInputCol(\"vector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
